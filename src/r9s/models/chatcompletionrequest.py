"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .audio import Audio, AudioTypedDict
from .message import Message, MessageTypedDict
from .responseformat import ResponseFormat, ResponseFormatTypedDict
from .streamoptions import StreamOptions, StreamOptionsTypedDict
from .tool import Tool, ToolTypedDict
import pydantic
from pydantic.functional_validators import AfterValidator
from r9s.types import BaseModel
from r9s.utils import validate_const
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


Modalities = Literal[
    "text",
    "audio",
]


ServiceTier = Literal[
    "auto",
    "default",
]


StopTypedDict = TypeAliasType("StopTypedDict", Union[str, List[str]])


Stop = TypeAliasType("Stop", Union[str, List[str]])


class ChatCompletionRequestFunctionTypedDict(TypedDict):
    name: NotRequired[str]


class ChatCompletionRequestFunction(BaseModel):
    name: Optional[str] = None


class TwoTypedDict(TypedDict):
    type: Literal["function"]
    function: NotRequired[ChatCompletionRequestFunctionTypedDict]


class Two(BaseModel):
    TYPE: Annotated[
        Annotated[
            Optional[Literal["function"]], AfterValidator(validate_const("function"))
        ],
        pydantic.Field(alias="type"),
    ] = "function"

    function: Optional[ChatCompletionRequestFunction] = None


One = Literal[
    "none",
    "auto",
    "required",
]


ChatCompletionRequestToolChoiceTypedDict = TypeAliasType(
    "ChatCompletionRequestToolChoiceTypedDict", Union[TwoTypedDict, One]
)


ChatCompletionRequestToolChoice = TypeAliasType(
    "ChatCompletionRequestToolChoice", Union[Two, One]
)


ReasoningEffort = Literal[
    "low",
    "medium",
    "high",
]
r"""Reasoning effort level for o1 series models (low, medium, high)"""


class ChatCompletionRequestTypedDict(TypedDict):
    model: str
    r"""Model name"""
    messages: List[MessageTypedDict]
    r"""Messages list"""
    frequency_penalty: NotRequired[float]
    logit_bias: NotRequired[Dict[str, float]]
    logprobs: NotRequired[bool]
    r"""When true, stream must be false (OpenAI constraint)"""
    top_logprobs: NotRequired[int]
    max_tokens: NotRequired[int]
    n: NotRequired[int]
    r"""Number of chat completion choices to generate"""
    modalities: NotRequired[List[Modalities]]
    r"""Output modality types. Use [\"text\", \"audio\"] for audio output"""
    audio: NotRequired[AudioTypedDict]
    presence_penalty: NotRequired[float]
    response_format: NotRequired[ResponseFormatTypedDict]
    seed: NotRequired[int]
    service_tier: NotRequired[ServiceTier]
    stop: NotRequired[StopTypedDict]
    stream: NotRequired[bool]
    stream_options: NotRequired[StreamOptionsTypedDict]
    temperature: NotRequired[float]
    top_p: NotRequired[float]
    top_k: NotRequired[int]
    r"""Top-k sampling parameter (non-OpenAI standard, model-specific)"""
    tools: NotRequired[List[ToolTypedDict]]
    tool_choice: NotRequired[ChatCompletionRequestToolChoiceTypedDict]
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use. Only valid when tools are specified."""
    user: NotRequired[str]
    r"""Unique identifier representing end-user for abuse monitoring"""
    reasoning_effort: NotRequired[ReasoningEffort]
    r"""Reasoning effort level for o1 series models (low, medium, high)"""
    max_completion_tokens: NotRequired[int]
    r"""Maximum number of tokens to generate in the completion (alternative to max_tokens, more precise)"""
    store: NotRequired[bool]
    r"""Whether to store the output for use in model distillation or evals"""
    metadata: NotRequired[Dict[str, Any]]
    r"""Custom metadata to attach to the request for tracking purposes"""


class ChatCompletionRequest(BaseModel):
    model: str
    r"""Model name"""

    messages: List[Message]
    r"""Messages list"""

    frequency_penalty: Optional[float] = None

    logit_bias: Optional[Dict[str, float]] = None

    logprobs: Optional[bool] = None
    r"""When true, stream must be false (OpenAI constraint)"""

    top_logprobs: Optional[int] = None

    max_tokens: Optional[int] = None

    n: Optional[int] = None
    r"""Number of chat completion choices to generate"""

    modalities: Optional[List[Modalities]] = None
    r"""Output modality types. Use [\"text\", \"audio\"] for audio output"""

    audio: Optional[Audio] = None

    presence_penalty: Optional[float] = None

    response_format: Optional[ResponseFormat] = None

    seed: Optional[int] = None

    service_tier: Optional[ServiceTier] = None

    stop: Optional[Stop] = None

    stream: Optional[bool] = False

    stream_options: Optional[StreamOptions] = None

    temperature: Optional[float] = None

    top_p: Optional[float] = None

    top_k: Optional[int] = None
    r"""Top-k sampling parameter (non-OpenAI standard, model-specific)"""

    tools: Optional[List[Tool]] = None

    tool_choice: Optional[ChatCompletionRequestToolChoice] = None

    parallel_tool_calls: Optional[bool] = None
    r"""Whether to enable parallel function calling during tool use. Only valid when tools are specified."""

    user: Optional[str] = None
    r"""Unique identifier representing end-user for abuse monitoring"""

    reasoning_effort: Optional[ReasoningEffort] = None
    r"""Reasoning effort level for o1 series models (low, medium, high)"""

    max_completion_tokens: Optional[int] = None
    r"""Maximum number of tokens to generate in the completion (alternative to max_tokens, more precise)"""

    store: Optional[bool] = None
    r"""Whether to store the output for use in model distillation or evals"""

    metadata: Optional[Dict[str, Any]] = None
    r"""Custom metadata to attach to the request for tracking purposes"""
