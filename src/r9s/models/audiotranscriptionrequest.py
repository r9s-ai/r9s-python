"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
import io
import pydantic
from r9s.types import BaseModel
from r9s.utils import FieldMetadata, MultipartFormMetadata
from typing import IO, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypedDict


class FileTypedDict(TypedDict):
    file_name: str
    content: Union[bytes, IO[bytes], io.BufferedReader]
    content_type: NotRequired[str]


class File(BaseModel):
    file_name: Annotated[
        str, pydantic.Field(alias="fileName"), FieldMetadata(multipart=True)
    ]

    content: Annotated[
        Union[bytes, IO[bytes], io.BufferedReader],
        pydantic.Field(alias=""),
        FieldMetadata(multipart=MultipartFormMetadata(content=True)),
    ]

    content_type: Annotated[
        Optional[str],
        pydantic.Field(alias="Content-Type"),
        FieldMetadata(multipart=True),
    ] = None


AudioTranscriptionRequestResponseFormat = Literal[
    "json",
    "text",
    "srt",
    "verbose_json",
    "vtt",
]
r"""Output format. Model support varies:
- whisper-1: Supports all formats (json, text, srt, verbose_json, vtt)
- gpt-4o-transcribe, gpt-4o-mini-transcribe: Only json and text

"""


TimestampGranularities = Literal[
    "word",
    "segment",
]


class AudioTranscriptionRequestTypedDict(TypedDict):
    file: FileTypedDict
    r"""Audio file to transcribe"""
    model: str
    r"""Model name"""
    language: NotRequired[str]
    r"""Audio language (ISO-639-1 format)"""
    prompt: NotRequired[str]
    r"""Optional text prompt"""
    response_format: NotRequired[AudioTranscriptionRequestResponseFormat]
    r"""Output format. Model support varies:
    - whisper-1: Supports all formats (json, text, srt, verbose_json, vtt)
    - gpt-4o-transcribe, gpt-4o-mini-transcribe: Only json and text

    """
    temperature: NotRequired[float]
    timestamp_granularities: NotRequired[List[TimestampGranularities]]
    r"""Timestamp granularity levels to include. Options: word, segment.
    **Important:** Only works when response_format is set to verbose_json.
    Note: segment timestamps have no additional latency, but word timestamps add latency.

    """


class AudioTranscriptionRequest(BaseModel):
    file: Annotated[File, FieldMetadata(multipart=MultipartFormMetadata(file=True))]
    r"""Audio file to transcribe"""

    model: Annotated[str, FieldMetadata(multipart=True)]
    r"""Model name"""

    language: Annotated[Optional[str], FieldMetadata(multipart=True)] = None
    r"""Audio language (ISO-639-1 format)"""

    prompt: Annotated[Optional[str], FieldMetadata(multipart=True)] = None
    r"""Optional text prompt"""

    response_format: Annotated[
        Optional[AudioTranscriptionRequestResponseFormat], FieldMetadata(multipart=True)
    ] = "json"
    r"""Output format. Model support varies:
    - whisper-1: Supports all formats (json, text, srt, verbose_json, vtt)
    - gpt-4o-transcribe, gpt-4o-mini-transcribe: Only json and text

    """

    temperature: Annotated[Optional[float], FieldMetadata(multipart=True)] = 0

    timestamp_granularities: Annotated[
        Optional[List[TimestampGranularities]], FieldMetadata(multipart=True)
    ] = None
    r"""Timestamp granularity levels to include. Options: word, segment.
    **Important:** Only works when response_format is set to verbose_json.
    Note: segment timestamps have no additional latency, but word timestamps add latency.

    """
