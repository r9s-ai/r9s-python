"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .message import Message, MessageTypedDict
from .responsetool import ResponseTool, ResponseToolTypedDict
from .toolchoice import ToolChoice, ToolChoiceTypedDict
import pydantic
from r9s.types import BaseModel
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


InputTypedDict = TypeAliasType("InputTypedDict", Union[str, List[MessageTypedDict]])
r"""Input content, required parameter. Can be:
- String: Single text input
- Message array: Structured conversation history

**Important limitations:**
- Messages only support basic fields (role, content, name)
- Does not support tool_calls, tool_call_id and other tool-related fields
- content field is required and cannot be null
- To use tools, define them in the top-level tools parameter; model will call them on first response

Note: Responses API has deprecated messages parameter, now uses input parameter uniformly

"""


Input = TypeAliasType("Input", Union[str, List[Message]])
r"""Input content, required parameter. Can be:
- String: Single text input
- Message array: Structured conversation history

**Important limitations:**
- Messages only support basic fields (role, content, name)
- Does not support tool_calls, tool_call_id and other tool-related fields
- content field is required and cannot be null
- To use tools, define them in the top-level tools parameter; model will call them on first response

Note: Responses API has deprecated messages parameter, now uses input parameter uniformly

"""


ResponseRequestModalities = Literal[
    "text",
    "audio",
]


ResponseRequest1 = Literal[
    "none",
    "auto",
    "required",
]


ResponseRequestToolChoiceTypedDict = TypeAliasType(
    "ResponseRequestToolChoiceTypedDict", Union[ToolChoiceTypedDict, ResponseRequest1]
)
r"""Tool selection strategy"""


ResponseRequestToolChoice = TypeAliasType(
    "ResponseRequestToolChoice", Union[ToolChoice, ResponseRequest1]
)
r"""Tool selection strategy"""


ResponseRequestType = Literal[
    "text",
    "json_object",
    "json_schema",
]
r"""The type of response format"""


class ResponseRequestFormatTypedDict(TypedDict):
    r"""An object specifying the format that the model must output. Setting to { \"type\": \"json_schema\", \"name\": \"...\", \"schema\": {...} } enables Structured Outputs which ensures the model will match your supplied JSON schema.
    Setting to { \"type\": \"json_object\" } enables JSON mode, which ensures the model generates valid JSON.

    """

    type: NotRequired[ResponseRequestType]
    r"""The type of response format"""
    name: NotRequired[str]
    r"""Name for the schema (required when type is json_schema)"""
    schema_: NotRequired[Dict[str, Any]]
    r"""JSON schema definition for structured outputs"""
    strict: NotRequired[bool]
    r"""Whether to enforce strict schema matching"""


class ResponseRequestFormat(BaseModel):
    r"""An object specifying the format that the model must output. Setting to { \"type\": \"json_schema\", \"name\": \"...\", \"schema\": {...} } enables Structured Outputs which ensures the model will match your supplied JSON schema.
    Setting to { \"type\": \"json_object\" } enables JSON mode, which ensures the model generates valid JSON.

    """

    type: Optional[ResponseRequestType] = None
    r"""The type of response format"""

    name: Optional[str] = None
    r"""Name for the schema (required when type is json_schema)"""

    schema_: Annotated[Optional[Dict[str, Any]], pydantic.Field(alias="schema")] = None
    r"""JSON schema definition for structured outputs"""

    strict: Optional[bool] = None
    r"""Whether to enforce strict schema matching"""


Verbosity = Literal[
    "low",
    "medium",
    "high",
]
r"""Verbosity level for the text output"""


class TextTypedDict(TypedDict):
    r"""Text output configuration"""

    format_: NotRequired[ResponseRequestFormatTypedDict]
    r"""An object specifying the format that the model must output. Setting to { \"type\": \"json_schema\", \"name\": \"...\", \"schema\": {...} } enables Structured Outputs which ensures the model will match your supplied JSON schema.
    Setting to { \"type\": \"json_object\" } enables JSON mode, which ensures the model generates valid JSON.

    """
    verbosity: NotRequired[Verbosity]
    r"""Verbosity level for the text output"""


class Text(BaseModel):
    r"""Text output configuration"""

    format_: Annotated[
        Optional[ResponseRequestFormat], pydantic.Field(alias="format")
    ] = None
    r"""An object specifying the format that the model must output. Setting to { \"type\": \"json_schema\", \"name\": \"...\", \"schema\": {...} } enables Structured Outputs which ensures the model will match your supplied JSON schema.
    Setting to { \"type\": \"json_object\" } enables JSON mode, which ensures the model generates valid JSON.

    """

    verbosity: Optional[Verbosity] = None
    r"""Verbosity level for the text output"""


Effort = Literal[
    "none",
    "minimal",
    "low",
    "medium",
    "high",
    "xhigh",
]
r"""The effort level for reasoning (none/minimal=fast, low/medium=balanced, high/xhigh=thorough)"""


class ReasoningTypedDict(TypedDict):
    r"""Configuration for reasoning models (e.g., o1, o3, gpt-5). Controls how the model uses reasoning tokens to \"think\" through the problem."""

    effort: NotRequired[Effort]
    r"""The effort level for reasoning (none/minimal=fast, low/medium=balanced, high/xhigh=thorough)"""
    summary: NotRequired[str]
    r"""Summary of reasoning approach"""


class Reasoning(BaseModel):
    r"""Configuration for reasoning models (e.g., o1, o3, gpt-5). Controls how the model uses reasoning tokens to \"think\" through the problem."""

    effort: Optional[Effort] = None
    r"""The effort level for reasoning (none/minimal=fast, low/medium=balanced, high/xhigh=thorough)"""

    summary: Optional[str] = None
    r"""Summary of reasoning approach"""


Truncation = Literal[
    "auto",
    "disabled",
]
r"""The truncation strategy to use for the model response.
- auto: If input exceeds context window, truncate by dropping items from beginning
- disabled: Request fails with 400 error if input exceeds context window (default)

"""


ResponseRequestStopTypedDict = TypeAliasType(
    "ResponseRequestStopTypedDict", Union[str, List[str]]
)
r"""Up to 4 sequences where the API will stop generating further tokens"""


ResponseRequestStop = TypeAliasType("ResponseRequestStop", Union[str, List[str]])
r"""Up to 4 sequences where the API will stop generating further tokens"""


class ResponseRequestTypedDict(TypedDict):
    model: str
    r"""Model name"""
    input: InputTypedDict
    r"""Input content, required parameter. Can be:
    - String: Single text input
    - Message array: Structured conversation history

    **Important limitations:**
    - Messages only support basic fields (role, content, name)
    - Does not support tool_calls, tool_call_id and other tool-related fields
    - content field is required and cannot be null
    - To use tools, define them in the top-level tools parameter; model will call them on first response

    Note: Responses API has deprecated messages parameter, now uses input parameter uniformly

    """
    instructions: NotRequired[str]
    r"""System-level instructions to guide model behavior and response style (similar to system message)"""
    temperature: NotRequired[float]
    r"""Controls output randomness, higher values mean more random"""
    top_p: NotRequired[float]
    r"""Nucleus sampling parameter, controls output diversity"""
    max_output_tokens: NotRequired[int]
    r"""Maximum number of tokens to generate"""
    stream: NotRequired[bool]
    r"""Whether to enable streaming"""
    modalities: NotRequired[List[ResponseRequestModalities]]
    r"""Response modality types"""
    tools: NotRequired[List[ResponseToolTypedDict]]
    r"""Available tools list (using flat format)"""
    tool_choice: NotRequired[ResponseRequestToolChoiceTypedDict]
    r"""Tool selection strategy"""
    parallel_tool_calls: NotRequired[bool]
    r"""Whether to enable parallel function calling during tool use. When false, ensures exactly zero or one tool is called."""
    text: NotRequired[TextTypedDict]
    r"""Text output configuration"""
    previous_response_id: NotRequired[str]
    r"""The ID of a previous response to continue the conversation from. This allows you to chain responses together and maintain conversation state.
    When using previous_response_id, the model will automatically have access to all previously produced reasoning items and conversation history.

    """
    store: NotRequired[bool]
    r"""Whether to store the generated model response for later retrieval via API.
    Defaults to true. Set to false to disable storage (required for ZDR organizations).

    """
    background: NotRequired[bool]
    r"""Whether to run the model response in the background asynchronously. Useful for long-running tasks."""
    reasoning: NotRequired[ReasoningTypedDict]
    r"""Configuration for reasoning models (e.g., o1, o3, gpt-5). Controls how the model uses reasoning tokens to \"think\" through the problem."""
    truncation: NotRequired[Truncation]
    r"""The truncation strategy to use for the model response.
    - auto: If input exceeds context window, truncate by dropping items from beginning
    - disabled: Request fails with 400 error if input exceeds context window (default)

    """
    stop: NotRequired[ResponseRequestStopTypedDict]
    r"""Up to 4 sequences where the API will stop generating further tokens"""
    metadata: NotRequired[Dict[str, Any]]
    r"""Additional metadata for tracking and organization purposes"""


class ResponseRequest(BaseModel):
    model: str
    r"""Model name"""

    input: Input
    r"""Input content, required parameter. Can be:
    - String: Single text input
    - Message array: Structured conversation history

    **Important limitations:**
    - Messages only support basic fields (role, content, name)
    - Does not support tool_calls, tool_call_id and other tool-related fields
    - content field is required and cannot be null
    - To use tools, define them in the top-level tools parameter; model will call them on first response

    Note: Responses API has deprecated messages parameter, now uses input parameter uniformly

    """

    instructions: Optional[str] = None
    r"""System-level instructions to guide model behavior and response style (similar to system message)"""

    temperature: Optional[float] = None
    r"""Controls output randomness, higher values mean more random"""

    top_p: Optional[float] = None
    r"""Nucleus sampling parameter, controls output diversity"""

    max_output_tokens: Optional[int] = None
    r"""Maximum number of tokens to generate"""

    stream: Optional[bool] = False
    r"""Whether to enable streaming"""

    modalities: Optional[List[ResponseRequestModalities]] = None
    r"""Response modality types"""

    tools: Optional[List[ResponseTool]] = None
    r"""Available tools list (using flat format)"""

    tool_choice: Optional[ResponseRequestToolChoice] = None
    r"""Tool selection strategy"""

    parallel_tool_calls: Optional[bool] = True
    r"""Whether to enable parallel function calling during tool use. When false, ensures exactly zero or one tool is called."""

    text: Optional[Text] = None
    r"""Text output configuration"""

    previous_response_id: Optional[str] = None
    r"""The ID of a previous response to continue the conversation from. This allows you to chain responses together and maintain conversation state.
    When using previous_response_id, the model will automatically have access to all previously produced reasoning items and conversation history.

    """

    store: Optional[bool] = True
    r"""Whether to store the generated model response for later retrieval via API.
    Defaults to true. Set to false to disable storage (required for ZDR organizations).

    """

    background: Optional[bool] = False
    r"""Whether to run the model response in the background asynchronously. Useful for long-running tasks."""

    reasoning: Optional[Reasoning] = None
    r"""Configuration for reasoning models (e.g., o1, o3, gpt-5). Controls how the model uses reasoning tokens to \"think\" through the problem."""

    truncation: Optional[Truncation] = "disabled"
    r"""The truncation strategy to use for the model response.
    - auto: If input exceeds context window, truncate by dropping items from beginning
    - disabled: Request fails with 400 error if input exceeds context window (default)

    """

    stop: Optional[ResponseRequestStop] = None
    r"""Up to 4 sequences where the API will stop generating further tokens"""

    metadata: Optional[Dict[str, Any]] = None
    r"""Additional metadata for tracking and organization purposes"""
