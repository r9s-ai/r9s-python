"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from .basesdk import BaseSDK
from enum import Enum
from r9s import errors, models, utils
from r9s._hooks import HookContext
from r9s.types import OptionalNullable, UNSET
from r9s.utils import eventstreaming
from r9s.utils.unmarshal_json_response import unmarshal_json_response
from typing import Any, Dict, List, Literal, Mapping, Optional, Union, overload


class CreateAcceptEnum(str, Enum):
    APPLICATION_JSON = "application/json"
    TEXT_EVENT_STREAM = "text/event-stream"


class Responses(BaseSDK):
    @overload
    def create(
        self,
        *,
        model: str,
        input: Union[models.Input, models.InputTypedDict],
        instructions: Optional[str] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_output_tokens: Optional[int] = None,
        stream: Union[Literal[False], None] = None,
        modalities: Optional[List[models.ResponseRequestModalities]] = None,
        tools: Optional[
            Union[List[models.ResponseTool], List[models.ResponseToolTypedDict]]
        ] = None,
        tool_choice: Optional[
            Union[
                models.ResponseRequestToolChoice,
                models.ResponseRequestToolChoiceTypedDict,
            ]
        ] = None,
        parallel_tool_calls: Optional[bool] = True,
        text: Optional[Union[models.Text, models.TextTypedDict]] = None,
        previous_response_id: Optional[str] = None,
        store: Optional[bool] = True,
        background: Optional[bool] = False,
        reasoning: Optional[Union[models.Reasoning, models.ReasoningTypedDict]] = None,
        truncation: Optional[models.Truncation] = "disabled",
        stop: Optional[
            Union[models.ResponseRequestStop, models.ResponseRequestStopTypedDict]
        ] = None,
        metadata: Optional[Dict[str, Any]] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ResponseObject:
        r"""Create response

        Create a response with streaming support. This endpoint corresponds to OpenAI's Responses API.


        :param model: Model name
        :param input: Input content, required parameter. Can be:
            - String: Single text input
            - Message array: Structured conversation history

            **Important limitations:**
            - Messages only support basic fields (role, content, name)
            - Does not support tool_calls, tool_call_id and other tool-related fields
            - content field is required and cannot be null
            - To use tools, define them in the top-level tools parameter; model will call them on first response

            Note: Responses API has deprecated messages parameter, now uses input parameter uniformly

        :param instructions: System-level instructions to guide model behavior and response style (similar to system message)
        :param temperature: Controls output randomness, higher values mean more random
        :param top_p: Nucleus sampling parameter, controls output diversity
        :param max_output_tokens: Maximum number of tokens to generate
        :param stream: Whether to enable streaming
        :param modalities: Response modality types
        :param tools: Available tools list (using flat format)
        :param tool_choice: Tool selection strategy
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use. When false, ensures exactly zero or one tool is called.
        :param text: Text output configuration
        :param previous_response_id: The ID of a previous response to continue the conversation from. This allows you to chain responses together and maintain conversation state.
            When using previous_response_id, the model will automatically have access to all previously produced reasoning items and conversation history.

        :param store: Whether to store the generated model response for later retrieval via API.
            Defaults to true. Set to false to disable storage (required for ZDR organizations).

        :param background: Whether to run the model response in the background asynchronously. Useful for long-running tasks.
        :param reasoning: Configuration for reasoning models (e.g., o1, o3, gpt-5). Controls how the model uses reasoning tokens to \"think\" through the problem.
        :param truncation: The truncation strategy to use for the model response.
            - auto: If input exceeds context window, truncate by dropping items from beginning
            - disabled: Request fails with 400 error if input exceeds context window (default)

        :param stop: Up to 4 sequences where the API will stop generating further tokens
        :param metadata: Additional metadata for tracking and organization purposes
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """

    @overload
    def create(
        self,
        *,
        model: str,
        input: Union[models.Input, models.InputTypedDict],
        instructions: Optional[str] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_output_tokens: Optional[int] = None,
        stream: Literal[True],
        modalities: Optional[List[models.ResponseRequestModalities]] = None,
        tools: Optional[
            Union[List[models.ResponseTool], List[models.ResponseToolTypedDict]]
        ] = None,
        tool_choice: Optional[
            Union[
                models.ResponseRequestToolChoice,
                models.ResponseRequestToolChoiceTypedDict,
            ]
        ] = None,
        parallel_tool_calls: Optional[bool] = True,
        text: Optional[Union[models.Text, models.TextTypedDict]] = None,
        previous_response_id: Optional[str] = None,
        store: Optional[bool] = True,
        background: Optional[bool] = False,
        reasoning: Optional[Union[models.Reasoning, models.ReasoningTypedDict]] = None,
        truncation: Optional[models.Truncation] = "disabled",
        stop: Optional[
            Union[models.ResponseRequestStop, models.ResponseRequestStopTypedDict]
        ] = None,
        metadata: Optional[Dict[str, Any]] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStream[models.ResponseStreamEvent]:
        r"""Create response

        Create a response with streaming support. This endpoint corresponds to OpenAI's Responses API.


        :param model: Model name
        :param input: Input content, required parameter. Can be:
            - String: Single text input
            - Message array: Structured conversation history

            **Important limitations:**
            - Messages only support basic fields (role, content, name)
            - Does not support tool_calls, tool_call_id and other tool-related fields
            - content field is required and cannot be null
            - To use tools, define them in the top-level tools parameter; model will call them on first response

            Note: Responses API has deprecated messages parameter, now uses input parameter uniformly

        :param instructions: System-level instructions to guide model behavior and response style (similar to system message)
        :param temperature: Controls output randomness, higher values mean more random
        :param top_p: Nucleus sampling parameter, controls output diversity
        :param max_output_tokens: Maximum number of tokens to generate
        :param stream: Whether to enable streaming
        :param modalities: Response modality types
        :param tools: Available tools list (using flat format)
        :param tool_choice: Tool selection strategy
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use. When false, ensures exactly zero or one tool is called.
        :param text: Text output configuration
        :param previous_response_id: The ID of a previous response to continue the conversation from. This allows you to chain responses together and maintain conversation state.
            When using previous_response_id, the model will automatically have access to all previously produced reasoning items and conversation history.

        :param store: Whether to store the generated model response for later retrieval via API.
            Defaults to true. Set to false to disable storage (required for ZDR organizations).

        :param background: Whether to run the model response in the background asynchronously. Useful for long-running tasks.
        :param reasoning: Configuration for reasoning models (e.g., o1, o3, gpt-5). Controls how the model uses reasoning tokens to \"think\" through the problem.
        :param truncation: The truncation strategy to use for the model response.
            - auto: If input exceeds context window, truncate by dropping items from beginning
            - disabled: Request fails with 400 error if input exceeds context window (default)

        :param stop: Up to 4 sequences where the API will stop generating further tokens
        :param metadata: Additional metadata for tracking and organization purposes
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """

    def create(
        self,
        *,
        model: str,
        input: Union[models.Input, models.InputTypedDict],
        instructions: Optional[str] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_output_tokens: Optional[int] = None,
        stream: Optional[bool] = False,
        modalities: Optional[List[models.ResponseRequestModalities]] = None,
        tools: Optional[
            Union[List[models.ResponseTool], List[models.ResponseToolTypedDict]]
        ] = None,
        tool_choice: Optional[
            Union[
                models.ResponseRequestToolChoice,
                models.ResponseRequestToolChoiceTypedDict,
            ]
        ] = None,
        parallel_tool_calls: Optional[bool] = True,
        text: Optional[Union[models.Text, models.TextTypedDict]] = None,
        previous_response_id: Optional[str] = None,
        store: Optional[bool] = True,
        background: Optional[bool] = False,
        reasoning: Optional[Union[models.Reasoning, models.ReasoningTypedDict]] = None,
        truncation: Optional[models.Truncation] = "disabled",
        stop: Optional[
            Union[models.ResponseRequestStop, models.ResponseRequestStopTypedDict]
        ] = None,
        metadata: Optional[Dict[str, Any]] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.CreateResponseResponse:
        r"""Create response

        Create a response with streaming support. This endpoint corresponds to OpenAI's Responses API.


        :param model: Model name
        :param input: Input content, required parameter. Can be:
            - String: Single text input
            - Message array: Structured conversation history

            **Important limitations:**
            - Messages only support basic fields (role, content, name)
            - Does not support tool_calls, tool_call_id and other tool-related fields
            - content field is required and cannot be null
            - To use tools, define them in the top-level tools parameter; model will call them on first response

            Note: Responses API has deprecated messages parameter, now uses input parameter uniformly

        :param instructions: System-level instructions to guide model behavior and response style (similar to system message)
        :param temperature: Controls output randomness, higher values mean more random
        :param top_p: Nucleus sampling parameter, controls output diversity
        :param max_output_tokens: Maximum number of tokens to generate
        :param stream: Whether to enable streaming
        :param modalities: Response modality types
        :param tools: Available tools list (using flat format)
        :param tool_choice: Tool selection strategy
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use. When false, ensures exactly zero or one tool is called.
        :param text: Text output configuration
        :param previous_response_id: The ID of a previous response to continue the conversation from. This allows you to chain responses together and maintain conversation state.
            When using previous_response_id, the model will automatically have access to all previously produced reasoning items and conversation history.

        :param store: Whether to store the generated model response for later retrieval via API.
            Defaults to true. Set to false to disable storage (required for ZDR organizations).

        :param background: Whether to run the model response in the background asynchronously. Useful for long-running tasks.
        :param reasoning: Configuration for reasoning models (e.g., o1, o3, gpt-5). Controls how the model uses reasoning tokens to \"think\" through the problem.
        :param truncation: The truncation strategy to use for the model response.
            - auto: If input exceeds context window, truncate by dropping items from beginning
            - disabled: Request fails with 400 error if input exceeds context window (default)

        :param stop: Up to 4 sequences where the API will stop generating further tokens
        :param metadata: Additional metadata for tracking and organization purposes
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ResponseRequest(
            model=model,
            input=utils.get_pydantic_model(input, models.Input),
            instructions=instructions,
            temperature=temperature,
            top_p=top_p,
            max_output_tokens=max_output_tokens,
            stream=stream,
            modalities=modalities,
            tools=utils.get_pydantic_model(tools, Optional[List[models.ResponseTool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ResponseRequestToolChoice]
            ),
            parallel_tool_calls=parallel_tool_calls,
            text=utils.get_pydantic_model(text, Optional[models.Text]),
            previous_response_id=previous_response_id,
            store=store,
            background=background,
            reasoning=utils.get_pydantic_model(reasoning, Optional[models.Reasoning]),
            truncation=truncation,
            stop=stop,
            metadata=metadata,
        )

        req = self._build_request(
            method="POST",
            path="/responses",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream" if stream else "application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ResponseRequest
            ),
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = self.do_request(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="createResponse",
                oauth2_scopes=None,
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=[
                "400",
                "401",
                "403",
                "422",
                "429",
                "4XX",
                "500",
                "503",
                "5XX",
            ],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            return unmarshal_json_response(
                models.ResponseObject, http_res, http_res_text
            )
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStream(
                http_res,
                lambda raw: utils.unmarshal_json(
                    raw, models.CreateResponseResponseBody
                ).data,
                sentinel="[DONE]",
                client_ref=self,
            )
        if utils.match_response(http_res, "400", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = unmarshal_json_response(
                errors.BadRequestErrorData, http_res, http_res_text
            )
            raise errors.BadRequestError(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "401", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = unmarshal_json_response(
                errors.AuthenticationErrorData, http_res, http_res_text
            )
            raise errors.AuthenticationError(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "403", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = unmarshal_json_response(
                errors.PermissionDeniedErrorData, http_res, http_res_text
            )
            raise errors.PermissionDeniedError(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = unmarshal_json_response(
                errors.UnprocessableEntityErrorData, http_res, http_res_text
            )
            raise errors.UnprocessableEntityError(
                response_data, http_res, http_res_text
            )
        if utils.match_response(http_res, "429", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = unmarshal_json_response(
                errors.RateLimitErrorData, http_res, http_res_text
            )
            raise errors.RateLimitError(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "500", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = unmarshal_json_response(
                errors.InternalServerErrorData, http_res, http_res_text
            )
            raise errors.InternalServerError(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "503", "application/json"):
            http_res_text = utils.stream_to_text(http_res)
            response_data = unmarshal_json_response(
                errors.ServiceUnavailableErrorData, http_res, http_res_text
            )
            raise errors.ServiceUnavailableError(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.R9SDefaultError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = utils.stream_to_text(http_res)
            raise errors.R9SDefaultError("API error occurred", http_res, http_res_text)

        http_res_text = utils.stream_to_text(http_res)
        raise errors.R9SDefaultError(
            "Unexpected response received", http_res, http_res_text
        )

    @overload
    async def create_async(
        self,
        *,
        model: str,
        input: Union[models.Input, models.InputTypedDict],
        instructions: Optional[str] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_output_tokens: Optional[int] = None,
        stream: Union[Literal[False], None] = None,
        modalities: Optional[List[models.ResponseRequestModalities]] = None,
        tools: Optional[
            Union[List[models.ResponseTool], List[models.ResponseToolTypedDict]]
        ] = None,
        tool_choice: Optional[
            Union[
                models.ResponseRequestToolChoice,
                models.ResponseRequestToolChoiceTypedDict,
            ]
        ] = None,
        parallel_tool_calls: Optional[bool] = True,
        text: Optional[Union[models.Text, models.TextTypedDict]] = None,
        previous_response_id: Optional[str] = None,
        store: Optional[bool] = True,
        background: Optional[bool] = False,
        reasoning: Optional[Union[models.Reasoning, models.ReasoningTypedDict]] = None,
        truncation: Optional[models.Truncation] = "disabled",
        stop: Optional[
            Union[models.ResponseRequestStop, models.ResponseRequestStopTypedDict]
        ] = None,
        metadata: Optional[Dict[str, Any]] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.ResponseObject:
        r"""Create response

        Create a response with streaming support. This endpoint corresponds to OpenAI's Responses API.


        :param model: Model name
        :param input: Input content, required parameter. Can be:
            - String: Single text input
            - Message array: Structured conversation history

            **Important limitations:**
            - Messages only support basic fields (role, content, name)
            - Does not support tool_calls, tool_call_id and other tool-related fields
            - content field is required and cannot be null
            - To use tools, define them in the top-level tools parameter; model will call them on first response

            Note: Responses API has deprecated messages parameter, now uses input parameter uniformly

        :param instructions: System-level instructions to guide model behavior and response style (similar to system message)
        :param temperature: Controls output randomness, higher values mean more random
        :param top_p: Nucleus sampling parameter, controls output diversity
        :param max_output_tokens: Maximum number of tokens to generate
        :param stream: Whether to enable streaming
        :param modalities: Response modality types
        :param tools: Available tools list (using flat format)
        :param tool_choice: Tool selection strategy
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use. When false, ensures exactly zero or one tool is called.
        :param text: Text output configuration
        :param previous_response_id: The ID of a previous response to continue the conversation from. This allows you to chain responses together and maintain conversation state.
            When using previous_response_id, the model will automatically have access to all previously produced reasoning items and conversation history.

        :param store: Whether to store the generated model response for later retrieval via API.
            Defaults to true. Set to false to disable storage (required for ZDR organizations).

        :param background: Whether to run the model response in the background asynchronously. Useful for long-running tasks.
        :param reasoning: Configuration for reasoning models (e.g., o1, o3, gpt-5). Controls how the model uses reasoning tokens to \"think\" through the problem.
        :param truncation: The truncation strategy to use for the model response.
            - auto: If input exceeds context window, truncate by dropping items from beginning
            - disabled: Request fails with 400 error if input exceeds context window (default)

        :param stop: Up to 4 sequences where the API will stop generating further tokens
        :param metadata: Additional metadata for tracking and organization purposes
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """

    @overload
    async def create_async(
        self,
        *,
        model: str,
        input: Union[models.Input, models.InputTypedDict],
        instructions: Optional[str] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_output_tokens: Optional[int] = None,
        stream: Literal[True],
        modalities: Optional[List[models.ResponseRequestModalities]] = None,
        tools: Optional[
            Union[List[models.ResponseTool], List[models.ResponseToolTypedDict]]
        ] = None,
        tool_choice: Optional[
            Union[
                models.ResponseRequestToolChoice,
                models.ResponseRequestToolChoiceTypedDict,
            ]
        ] = None,
        parallel_tool_calls: Optional[bool] = True,
        text: Optional[Union[models.Text, models.TextTypedDict]] = None,
        previous_response_id: Optional[str] = None,
        store: Optional[bool] = True,
        background: Optional[bool] = False,
        reasoning: Optional[Union[models.Reasoning, models.ReasoningTypedDict]] = None,
        truncation: Optional[models.Truncation] = "disabled",
        stop: Optional[
            Union[models.ResponseRequestStop, models.ResponseRequestStopTypedDict]
        ] = None,
        metadata: Optional[Dict[str, Any]] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> eventstreaming.EventStreamAsync[models.ResponseStreamEvent]:
        r"""Create response

        Create a response with streaming support. This endpoint corresponds to OpenAI's Responses API.


        :param model: Model name
        :param input: Input content, required parameter. Can be:
            - String: Single text input
            - Message array: Structured conversation history

            **Important limitations:**
            - Messages only support basic fields (role, content, name)
            - Does not support tool_calls, tool_call_id and other tool-related fields
            - content field is required and cannot be null
            - To use tools, define them in the top-level tools parameter; model will call them on first response

            Note: Responses API has deprecated messages parameter, now uses input parameter uniformly

        :param instructions: System-level instructions to guide model behavior and response style (similar to system message)
        :param temperature: Controls output randomness, higher values mean more random
        :param top_p: Nucleus sampling parameter, controls output diversity
        :param max_output_tokens: Maximum number of tokens to generate
        :param stream: Whether to enable streaming
        :param modalities: Response modality types
        :param tools: Available tools list (using flat format)
        :param tool_choice: Tool selection strategy
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use. When false, ensures exactly zero or one tool is called.
        :param text: Text output configuration
        :param previous_response_id: The ID of a previous response to continue the conversation from. This allows you to chain responses together and maintain conversation state.
            When using previous_response_id, the model will automatically have access to all previously produced reasoning items and conversation history.

        :param store: Whether to store the generated model response for later retrieval via API.
            Defaults to true. Set to false to disable storage (required for ZDR organizations).

        :param background: Whether to run the model response in the background asynchronously. Useful for long-running tasks.
        :param reasoning: Configuration for reasoning models (e.g., o1, o3, gpt-5). Controls how the model uses reasoning tokens to \"think\" through the problem.
        :param truncation: The truncation strategy to use for the model response.
            - auto: If input exceeds context window, truncate by dropping items from beginning
            - disabled: Request fails with 400 error if input exceeds context window (default)

        :param stop: Up to 4 sequences where the API will stop generating further tokens
        :param metadata: Additional metadata for tracking and organization purposes
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """

    async def create_async(
        self,
        *,
        model: str,
        input: Union[models.Input, models.InputTypedDict],
        instructions: Optional[str] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_output_tokens: Optional[int] = None,
        stream: Optional[bool] = False,
        modalities: Optional[List[models.ResponseRequestModalities]] = None,
        tools: Optional[
            Union[List[models.ResponseTool], List[models.ResponseToolTypedDict]]
        ] = None,
        tool_choice: Optional[
            Union[
                models.ResponseRequestToolChoice,
                models.ResponseRequestToolChoiceTypedDict,
            ]
        ] = None,
        parallel_tool_calls: Optional[bool] = True,
        text: Optional[Union[models.Text, models.TextTypedDict]] = None,
        previous_response_id: Optional[str] = None,
        store: Optional[bool] = True,
        background: Optional[bool] = False,
        reasoning: Optional[Union[models.Reasoning, models.ReasoningTypedDict]] = None,
        truncation: Optional[models.Truncation] = "disabled",
        stop: Optional[
            Union[models.ResponseRequestStop, models.ResponseRequestStopTypedDict]
        ] = None,
        metadata: Optional[Dict[str, Any]] = None,
        retries: OptionalNullable[utils.RetryConfig] = UNSET,
        server_url: Optional[str] = None,
        timeout_ms: Optional[int] = None,
        http_headers: Optional[Mapping[str, str]] = None,
    ) -> models.CreateResponseResponse:
        r"""Create response

        Create a response with streaming support. This endpoint corresponds to OpenAI's Responses API.


        :param model: Model name
        :param input: Input content, required parameter. Can be:
            - String: Single text input
            - Message array: Structured conversation history

            **Important limitations:**
            - Messages only support basic fields (role, content, name)
            - Does not support tool_calls, tool_call_id and other tool-related fields
            - content field is required and cannot be null
            - To use tools, define them in the top-level tools parameter; model will call them on first response

            Note: Responses API has deprecated messages parameter, now uses input parameter uniformly

        :param instructions: System-level instructions to guide model behavior and response style (similar to system message)
        :param temperature: Controls output randomness, higher values mean more random
        :param top_p: Nucleus sampling parameter, controls output diversity
        :param max_output_tokens: Maximum number of tokens to generate
        :param stream: Whether to enable streaming
        :param modalities: Response modality types
        :param tools: Available tools list (using flat format)
        :param tool_choice: Tool selection strategy
        :param parallel_tool_calls: Whether to enable parallel function calling during tool use. When false, ensures exactly zero or one tool is called.
        :param text: Text output configuration
        :param previous_response_id: The ID of a previous response to continue the conversation from. This allows you to chain responses together and maintain conversation state.
            When using previous_response_id, the model will automatically have access to all previously produced reasoning items and conversation history.

        :param store: Whether to store the generated model response for later retrieval via API.
            Defaults to true. Set to false to disable storage (required for ZDR organizations).

        :param background: Whether to run the model response in the background asynchronously. Useful for long-running tasks.
        :param reasoning: Configuration for reasoning models (e.g., o1, o3, gpt-5). Controls how the model uses reasoning tokens to \"think\" through the problem.
        :param truncation: The truncation strategy to use for the model response.
            - auto: If input exceeds context window, truncate by dropping items from beginning
            - disabled: Request fails with 400 error if input exceeds context window (default)

        :param stop: Up to 4 sequences where the API will stop generating further tokens
        :param metadata: Additional metadata for tracking and organization purposes
        :param retries: Override the default retry configuration for this method
        :param server_url: Override the default server URL for this method
        :param timeout_ms: Override the default request timeout configuration for this method in milliseconds
        :param accept_header_override: Override the default accept header for this method
        :param http_headers: Additional headers to set or replace on requests.
        """
        base_url = None
        url_variables = None
        if timeout_ms is None:
            timeout_ms = self.sdk_configuration.timeout_ms

        if server_url is not None:
            base_url = server_url
        else:
            base_url = self._get_url(base_url, url_variables)

        request = models.ResponseRequest(
            model=model,
            input=utils.get_pydantic_model(input, models.Input),
            instructions=instructions,
            temperature=temperature,
            top_p=top_p,
            max_output_tokens=max_output_tokens,
            stream=stream,
            modalities=modalities,
            tools=utils.get_pydantic_model(tools, Optional[List[models.ResponseTool]]),
            tool_choice=utils.get_pydantic_model(
                tool_choice, Optional[models.ResponseRequestToolChoice]
            ),
            parallel_tool_calls=parallel_tool_calls,
            text=utils.get_pydantic_model(text, Optional[models.Text]),
            previous_response_id=previous_response_id,
            store=store,
            background=background,
            reasoning=utils.get_pydantic_model(reasoning, Optional[models.Reasoning]),
            truncation=truncation,
            stop=stop,
            metadata=metadata,
        )

        req = self._build_request_async(
            method="POST",
            path="/responses",
            base_url=base_url,
            url_variables=url_variables,
            request=request,
            request_body_required=True,
            request_has_path_params=False,
            request_has_query_params=True,
            user_agent_header="user-agent",
            accept_header_value="text/event-stream" if stream else "application/json",
            http_headers=http_headers,
            security=self.sdk_configuration.security,
            get_serialized_body=lambda: utils.serialize_request_body(
                request, False, False, "json", models.ResponseRequest
            ),
            allow_empty_value=None,
            timeout_ms=timeout_ms,
        )

        if retries == UNSET:
            if self.sdk_configuration.retry_config is not UNSET:
                retries = self.sdk_configuration.retry_config

        retry_config = None
        if isinstance(retries, utils.RetryConfig):
            retry_config = (retries, ["429", "500", "502", "503", "504"])

        http_res = await self.do_request_async(
            hook_ctx=HookContext(
                config=self.sdk_configuration,
                base_url=base_url or "",
                operation_id="createResponse",
                oauth2_scopes=None,
                security_source=self.sdk_configuration.security,
            ),
            request=req,
            error_status_codes=[
                "400",
                "401",
                "403",
                "422",
                "429",
                "4XX",
                "500",
                "503",
                "5XX",
            ],
            stream=True,
            retry_config=retry_config,
        )

        response_data: Any = None
        if utils.match_response(http_res, "200", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            return unmarshal_json_response(
                models.ResponseObject, http_res, http_res_text
            )
        if utils.match_response(http_res, "200", "text/event-stream"):
            return eventstreaming.EventStreamAsync(
                http_res,
                lambda raw: utils.unmarshal_json(
                    raw, models.CreateResponseResponseBody
                ).data,
                sentinel="[DONE]",
                client_ref=self,
            )
        if utils.match_response(http_res, "400", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = unmarshal_json_response(
                errors.BadRequestErrorData, http_res, http_res_text
            )
            raise errors.BadRequestError(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "401", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = unmarshal_json_response(
                errors.AuthenticationErrorData, http_res, http_res_text
            )
            raise errors.AuthenticationError(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "403", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = unmarshal_json_response(
                errors.PermissionDeniedErrorData, http_res, http_res_text
            )
            raise errors.PermissionDeniedError(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "422", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = unmarshal_json_response(
                errors.UnprocessableEntityErrorData, http_res, http_res_text
            )
            raise errors.UnprocessableEntityError(
                response_data, http_res, http_res_text
            )
        if utils.match_response(http_res, "429", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = unmarshal_json_response(
                errors.RateLimitErrorData, http_res, http_res_text
            )
            raise errors.RateLimitError(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "500", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = unmarshal_json_response(
                errors.InternalServerErrorData, http_res, http_res_text
            )
            raise errors.InternalServerError(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "503", "application/json"):
            http_res_text = await utils.stream_to_text_async(http_res)
            response_data = unmarshal_json_response(
                errors.ServiceUnavailableErrorData, http_res, http_res_text
            )
            raise errors.ServiceUnavailableError(response_data, http_res, http_res_text)
        if utils.match_response(http_res, "4XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.R9SDefaultError("API error occurred", http_res, http_res_text)
        if utils.match_response(http_res, "5XX", "*"):
            http_res_text = await utils.stream_to_text_async(http_res)
            raise errors.R9SDefaultError("API error occurred", http_res, http_res_text)

        http_res_text = await utils.stream_to_text_async(http_res)
        raise errors.R9SDefaultError(
            "Unexpected response received", http_res, http_res_text
        )
